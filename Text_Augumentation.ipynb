{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5535dd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gdk14/nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gdk14/nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(augmented_text)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Augmenting each medical transcript\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m augmented_transcripts \u001b[38;5;241m=\u001b[39m [synonym_replacement(transcript) \u001b[38;5;28;01mfor\u001b[39;00m transcript \u001b[38;5;129;01min\u001b[39;00m medical_transcripts]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Print original and augmented transcripts\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Transcripts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(augmented_text)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Augmenting each medical transcript\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m augmented_transcripts \u001b[38;5;241m=\u001b[39m [synonym_replacement(transcript) \u001b[38;5;28;01mfor\u001b[39;00m transcript \u001b[38;5;129;01min\u001b[39;00m medical_transcripts]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Print original and augmented transcripts\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Transcripts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m, in \u001b[0;36msynonym_replacement\u001b[1;34m(text, num_replacements)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:  \u001b[38;5;66;03m# Probability of replacing each word\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m         synonyms \u001b[38;5;241m=\u001b[39m get_synonyms(word)\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m synonyms:\n\u001b[0;32m     28\u001b[0m             replacement \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(synonyms)\n",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m, in \u001b[0;36mget_synonyms\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_synonyms\u001b[39m(word):\n\u001b[0;32m     14\u001b[0m     synonyms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m syn \u001b[38;5;129;01min\u001b[39;00m wordnet\u001b[38;5;241m.\u001b[39msynsets(word):\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m lemma \u001b[38;5;129;01min\u001b[39;00m syn\u001b[38;5;241m.\u001b[39mlemmas():\n\u001b[0;32m     17\u001b[0m             synonyms\u001b[38;5;241m.\u001b[39madd(lemma\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gdk14/nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to get synonyms of a word using NLTK WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to perform synonym replacement in a text\n",
    "def synonym_replacement(text, num_replacements=1):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    augmented_text = []\n",
    "    for word in words:\n",
    "        if random.random() < 0.5:  # Probability of replacing each word\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms)\n",
    "                augmented_text.append(replacement)\n",
    "            else:\n",
    "                augmented_text.append(word)\n",
    "        else:\n",
    "            augmented_text.append(word)\n",
    "    return ' '.join(augmented_text)\n",
    "\n",
    "# Augmenting each medical transcript\n",
    "augmented_transcripts = [synonym_replacement(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcca2db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Transcripts:\n",
      "-  Patient presented with symptoms of cough and shortness of breath.\n",
      "-  Physical examination revealed elevated temperature and wheezing.\n",
      "-  Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\n",
      "\n",
      "Augmented Transcripts:\n",
      "-  Patient presented with symptoms of cough and shortness of breath. additional_word\n",
      "-  Physical additional_word examination revealed elevated temperature and wheezing.\n",
      "-  Diagnosis confirmed as bronchitis, prescribed antibiotics additional_word and inhaler.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to perform random insertion in a text\n",
    "def random_insertion(text, num_insertions=1):\n",
    "    words = text.split()\n",
    "    for _ in range(num_insertions):\n",
    "        insertion_point = random.randint(0, len(words))\n",
    "        insertion_word = \"additional_word\"\n",
    "        words.insert(insertion_point, insertion_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Augmenting each medical transcript\n",
    "augmented_transcripts = [random_insertion(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876001f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to perform random deletion in a text\n",
    "def random_deletion(text, probability=0.5):\n",
    "    words = text.split()\n",
    "    remaining_words = [word for word in words if random.random() > probability]\n",
    "    return ' '.join(remaining_words)\n",
    "\n",
    "# Augmenting each medical transcript\n",
    "augmented_transcripts = [random_deletion(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd1f25db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Transcripts:\n",
      "-  Patient presented with symptoms of cough and shortness of breath.\n",
      "-  Physical examination revealed elevated temperature and wheezing.\n",
      "-  Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\n",
      "\n",
      "Augmented Transcripts:\n",
      "-  Patient presented with symptoms of shortness and cough of breath.\n",
      "-  Physical examination revealed wheezing. temperature and elevated\n",
      "-  Diagnosis confirmed and bronchitis, prescribed antibiotics as inhaler.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to perform random swap in a text\n",
    "def random_swap(text, num_swaps=1):\n",
    "    words = text.split()\n",
    "    for _ in range(num_swaps):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Augmenting each medical transcript\n",
    "augmented_transcripts = [random_swap(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e61fa53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googletrans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googletrans'"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Function to perform back-translation\n",
    "def back_translate(text, src_lang='en', target_lang='fr'):\n",
    "    try:\n",
    "        # Translate to the target language\n",
    "        translated_text = translator.translate(text, src=src_lang, dest=target_lang).text\n",
    "        # Introduce artificial delay to avoid rate limiting\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        # Translate back to the source language\n",
    "        back_translated_text = translator.translate(translated_text, src=target_lang, dest=src_lang).text\n",
    "        return back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Perform back-translation on each transcript with retry and exponential backoff\n",
    "augmented_transcripts = []\n",
    "for transcript in medical_transcripts:\n",
    "    retries = 3\n",
    "    for _ in range(retries):\n",
    "        translated_text = back_translate(transcript)\n",
    "        if translated_text is not None:\n",
    "            augmented_transcripts.append(translated_text)\n",
    "            break\n",
    "        else:\n",
    "            # Retry with exponential backoff\n",
    "            delay = 2 ** _  # Exponential backoff\n",
    "            time.sleep(delay)\n",
    "    else:\n",
    "        # If all retries fail, use original text\n",
    "        augmented_transcripts.append(transcript)\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for i, transcript in enumerate(medical_transcripts):\n",
    "    print(f\"{i+1}. {transcript}\")\n",
    "\n",
    "print(\"\\nAugmented Transcripts:\")\n",
    "for i, transcript in enumerate(augmented_transcripts):\n",
    "    print(f\"{i+1}. {transcript}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e1f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Using cached googletrans-4.0.0rc1-py3-none-any.whl\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.1)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\siva7\\appdata\\roaming\\python\\python39\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Installing collected packages: googletrans\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 3.0.0\n",
      "    Uninstalling googletrans-3.0.0:\n",
      "      Successfully uninstalled googletrans-3.0.0\n",
      "Successfully installed googletrans-4.0.0rc1\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a615be",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gdk14/nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gdk14/nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(paraphrased_text)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Augmenting each medical transcript\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m augmented_transcripts \u001b[38;5;241m=\u001b[39m [paraphrase(transcript) \u001b[38;5;28;01mfor\u001b[39;00m transcript \u001b[38;5;129;01min\u001b[39;00m medical_transcripts]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Print original and augmented transcripts\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Transcripts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(paraphrased_text)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Augmenting each medical transcript\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m augmented_transcripts \u001b[38;5;241m=\u001b[39m [paraphrase(transcript) \u001b[38;5;28;01mfor\u001b[39;00m transcript \u001b[38;5;129;01min\u001b[39;00m medical_transcripts]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Print original and augmented transcripts\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Transcripts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mparaphrase\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     23\u001b[0m paraphrased_text \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m---> 25\u001b[0m     synonyms \u001b[38;5;241m=\u001b[39m get_synonyms(word)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synonyms:\n\u001b[0;32m     27\u001b[0m         paraphrased_word \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(synonyms)\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mget_synonyms\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_synonyms\u001b[39m(word):\n\u001b[0;32m     14\u001b[0m     synonyms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m syn \u001b[38;5;129;01min\u001b[39;00m wordnet\u001b[38;5;241m.\u001b[39msynsets(word):\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m lemma \u001b[38;5;129;01min\u001b[39;00m syn\u001b[38;5;241m.\u001b[39mlemmas():\n\u001b[0;32m     17\u001b[0m             synonyms\u001b[38;5;241m.\u001b[39madd(lemma\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gdk14/nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gdk14\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to get synonyms of a word using NLTK WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to perform paraphrasing in a text\n",
    "def paraphrase(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    paraphrased_text = []\n",
    "    for word in words:\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            paraphrased_word = random.choice(synonyms)\n",
    "            paraphrased_text.append(paraphrased_word)\n",
    "        else:\n",
    "            paraphrased_text.append(word)\n",
    "    return ' '.join(paraphrased_text)\n",
    "\n",
    "# Augmenting each medical transcript\n",
    "augmented_transcripts = [paraphrase(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6601e63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Transcripts:\n",
      "-  Patient presented with symptoms of cough and shortness of breath.\n",
      "-  Physical examination revealed elevated temperature and wheezing.\n",
      "-  Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\n",
      "\n",
      "Augmented Transcripts:\n",
      "-  affected_role introduce with symptom of cough and brusqueness of breath .\n",
      "-  strong-arm scrutiny let_out kick_upstairs temperature and wheezy .\n",
      "-  diagnosis support Eastern_Samoa bronchitis , order antibiotic_drug and inhaler .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to get synonyms of a word using NLTK WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to perform paraphrasing in a text\n",
    "def paraphrase(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    paraphrased_text = []\n",
    "    for word in words:\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            paraphrased_word = random.choice(synonyms)\n",
    "            paraphrased_text.append(paraphrased_word)\n",
    "        else:\n",
    "            paraphrased_text.append(word)\n",
    "    return ' '.join(paraphrased_text)\n",
    "\n",
    "# Augmenting each medical transcript\n",
    "augmented_transcripts = [paraphrase(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9bdb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Transcripts:\n",
      "1. shortness Patient of cough with symptoms of presented and breath .\n",
      "2. . , inhaler and Diagnosis as antibiotics prescribed bronchitis confirmed\n",
      "3. . and revealed wheezing examination elevated Physical temperature\n",
      "4. shortness with of and Patient presented breath symptoms of cough .\n",
      "5. confirmed inhaler and , . prescribed bronchitis Diagnosis as antibiotics\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "# Example medical transcripts data\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to generate new sentences based on existing ones\n",
    "def generate_text(sentences, num_sentences=5):\n",
    "    generated_sentences = []\n",
    "\n",
    "    for _ in range(num_sentences):\n",
    "        # Randomly select a sentence from the existing transcripts\n",
    "        random_sentence = random.choice(sentences)\n",
    "\n",
    "        # Tokenize the sentence into words\n",
    "        words = nltk.word_tokenize(random_sentence)\n",
    "\n",
    "        # Shuffle the words to create variations\n",
    "        random.shuffle(words)\n",
    "\n",
    "        # Join the shuffled words to form a new sentence\n",
    "        new_sentence = ' '.join(words)\n",
    "        generated_sentences.append(new_sentence)\n",
    "\n",
    "    return generated_sentences\n",
    "\n",
    "# Generate new medical transcripts\n",
    "generated_transcripts = generate_text(medical_transcripts)\n",
    "\n",
    "# Print the generated transcripts\n",
    "print(\"Generated Transcripts:\")\n",
    "for i, transcript in enumerate(generated_transcripts, 1):\n",
    "    print(f\"{i}. {transcript}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0269a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gdk14\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792ca2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Original Transcripts:\n",
      "-  Patient presented with symptoms of cough and shortness of breath.\n",
      "-  Physical examination revealed elevated temperature and wheezing.\n",
      "-  Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\n",
      "\n",
      "Augmented Transcripts with Word Embedding Interpolation:\n",
      "-  By_Nick_Mashiter presented Schlorff symptoms of cough and shortness of breath.\n",
      "-  Gymnastics_Spectacular examination Ovidiu_Rom forced_disappearances_extralegal temperature and wheezing.\n",
      "-  Diagnosis confirmed Darya_Zhukova bronchitis, orthopedist_Dr._Xavier_Duralde antibiotics and inhaler.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained word embeddings (word2vec)\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to perform word embedding interpolation\n",
    "def word_embedding_interpolation(text, alpha=0.5):\n",
    "    words = text.split()\n",
    "    interpolated_text = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for word in words:\n",
    "        # Check if word exists in the word embeddings vocabulary\n",
    "        if word in word_vectors:\n",
    "            # Get word embeddings\n",
    "            word_vector = word_vectors[word]\n",
    "\n",
    "            # Generate a random vector for interpolation\n",
    "            random_vector = np.random.rand(len(word_vector))\n",
    "\n",
    "            # Interpolate between word vector and random vector\n",
    "            interpolated_vector = (1 - alpha) * word_vector + alpha * random_vector\n",
    "\n",
    "            # Find the closest word in the embeddings space to the interpolated vector\n",
    "            closest_word = word_vectors.similar_by_vector(interpolated_vector)[0][0]\n",
    "            interpolated_text.append(closest_word)\n",
    "        else:\n",
    "            interpolated_text.append(word)\n",
    "\n",
    "    return ' '.join(interpolated_text)\n",
    "\n",
    "# Augmenting each medical transcript with word embedding interpolation\n",
    "augmented_transcripts = [word_embedding_interpolation(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts with Word Embedding Interpolation:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37269be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Transcripts:\n",
      "-  Patient presented with symptoms of cough and shortness of breath.\n",
      "-  Physical examination revealed elevated temperature and wheezing.\n",
      "-  Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\n",
      "\n",
      "Augmented Transcripts with Text Rotation Interpolation:\n",
      "-  cough and shortness of breath. Patient presented with symptoms of\n",
      "-  temperature and wheezing. Physical examination revealed elevated\n",
      "-  prescribed antibiotics and inhaler. Diagnosis confirmed as bronchitis,\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to perform text rotation interpolation\n",
    "def text_rotation_interpolation(text, rotation_factor=0.5):\n",
    "    words = text.split()\n",
    "    num_rotations = round(len(words) * rotation_factor)\n",
    "    rotated_text = words[num_rotations:] + words[:num_rotations]\n",
    "    return ' '.join(rotated_text)\n",
    "\n",
    "# Augmenting each medical transcript with text rotation interpolation\n",
    "augmented_transcripts = [text_rotation_interpolation(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts with Text Rotation Interpolation:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2d670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Transcripts:\n",
      "-  Patient presented with symptoms of cough and shortness of breath.\n",
      "-  Physical examination revealed elevated temperature and wheezing.\n",
      "-  Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\n",
      "\n",
      "Augmented Transcripts with Text Masking Interpolation:\n",
      "-  MASK presented MASK symptoms MASK cough and shortness MASK MASK\n",
      "-  MASK MASK revealed MASK temperature and MASK\n",
      "-  Diagnosis confirmed MASK bronchitis, MASK MASK and MASK\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample medical transcripts\n",
    "medical_transcripts = [\n",
    "    \"Patient presented with symptoms of cough and shortness of breath.\",\n",
    "    \"Physical examination revealed elevated temperature and wheezing.\",\n",
    "    \"Diagnosis confirmed as bronchitis, prescribed antibiotics and inhaler.\"\n",
    "]\n",
    "\n",
    "# Function to perform text masking interpolation\n",
    "def text_masking_interpolation(text, masking_factor=0.5):\n",
    "    words = text.split()\n",
    "    num_words_to_mask = round(len(words) * masking_factor)\n",
    "    masked_indices = random.sample(range(len(words)), num_words_to_mask)\n",
    "    masked_text = [word if idx not in masked_indices else 'MASK' for idx, word in enumerate(words)]\n",
    "    return ' '.join(masked_text)\n",
    "\n",
    "# Augmenting each medical transcript with text masking interpolation\n",
    "augmented_transcripts = [text_masking_interpolation(transcript) for transcript in medical_transcripts]\n",
    "\n",
    "# Print original and augmented transcripts\n",
    "print(\"Original Transcripts:\")\n",
    "for transcript in medical_transcripts:\n",
    "    print(\"- \", transcript)\n",
    "\n",
    "print(\"\\nAugmented Transcripts with Text Masking Interpolation:\")\n",
    "for augmented_transcript in augmented_transcripts:\n",
    "    print(\"- \", augmented_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482472c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
